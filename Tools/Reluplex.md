SMT solver for  theory of linear real arithmetic with ReLU constraints.
ReLU (Rectigied Linear Unit), are a specific kind of activation function used in deep neural networks (DNNs).

Artifact of CAV '17 paper: https://github.com/guykatzz/ReluplexCav2017

Note: The reluplex algorithm is nowadays implemented (as well as other techniques) in [Marabou](Marabou.md)